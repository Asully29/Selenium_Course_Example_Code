# Automating Your Test Runs

You'll probably get a lot of mileage out of your test suite in its current form if you just run things from your computer, look at the results, and inform people when there are issues. But that only helps you solve part of the problem.

The real goal in all of this is to find issues reliably, quickly, and continuously. We've built things to be reliable and quick. Now we need to make them run continuously -- and ideally in sync with the development workflow you are a part of. In order to do that, we want to use a Continuous Integration (CI) server.

## A Continuous Integration Primer

Continous Integration (a.k.a. CI) is the practice of merging code that is actively being worked on into a shared mainline (e.g., trunk or master) as often as possible (e.g., several times a day). This is with the hopes of finding issues early and avoiding merging and integration issues that are not only considered a special kind of hell, but can dramatically slow the time it takes to release software.

The use of a CI server (a.k.a. build server) enables this practice to be automated, and to have tests run as part of the work flow. The lion's share of tests that are typically run on a CI Server are unit (and potentially integration) tests. But we can very easily add in our automated acceptance tests.

There are numerous CI Servers available for use today, most notably:

+ [Bamboo](https://www.atlassian.com/software/bamboo)
+ [Jenkins](http://jenkins-ci.org/)
+ [Tddium](https://www.tddium.com/)
+ [TravisCI](https://travis-ci.org/)

## Part 1: Tagging & Workflow

In order to get the most out of our test runs in a CI environment, we want to break up our test suite into small, relevant chunks and have separate jobs for each. This helps keep test runs fast (so people on your team will care about them) and informative. [Gojko Adzic refers to these as 'Test Packs'](http://gojko.net/2011/05/16/setting-up-a-build-pipeline-in-a-legacy-environment/).

The workflow is pretty straightforward. The CI Server pulls in the latest code, merges it, and runs unit tests. We then have the CI Server kick off a new job to deploy to a test server and run a subset of our acceptance tests -- our critical ones (e.g., smoke or sanity tests). Assuming those pass, we can have another job run the remaining tests after that -- our longer running tests. [Adam Goucher refers to this strategy as a 'shallow' and 'deep' tagging model](https://github.com/adamgoucher/rspec-selenium-pageobjects#tags).

To demonstrate this, let's tag our tests and update our rake tasks to use them.

### An Example

RSpec comes built in with tagging support. It's a simple matter of adding a key/value pair to denote what you want. You can place it on individual tests, or a group of tests -- using as many tags as you want (separating them with commas).

Let's add some to our specs -- following Adam Goucher's shallow and deep approach.

```ruby
# filename: spec/dynamic_loading_spec.rb

require_relative 'spec_helper'
require_relative '../pages/dynamic_loading'

describe 'Dynamic Loading', depth: 'deep' do
...
```

```ruby
# filename: spec/login_spec.rb

require_relative 'spec_helper'
require_relative '../pages/login'

describe 'Login', depth: 'shallow' do
...
```

If we wanted to apply this tag directly to a test, then it would look like this:

```ruby
it 'succeeded', depth: 'shallow' do
```

To run tests based on a specific tag, we will need to pass in an additional argument to RSpec. It starts with `--tag` and accepts a tag's key/value pair (e.g., `--tag depth:shallow` or `--tag depth:deep`).

Let's update our 'Rakefile' tasks to handle this.

```ruby
# filename: Rakefile

def launch_in_parallel(config_file)
  system("parallel_rspec #{'-n ' + ENV['processes'] if ENV['processes']} --test-options '-r ./#{config_file} --order random #{'--tag ' + ENV['tag'] if ENV['tag']}' spec")
end
...
```

By updating our `launch_in_parallel` method to use a conditional based on the existence of the tag environment variable (`ENV['tag']`), we are able to dynamically alter our execution string at run time without having to change any of our tasks.

We just have to specify the tags at run time before our rake task.

```sh
tag='depth:shallow' rake local:firefox
```

## Part 2: Reporting

In order to make our test output useful for a CI Server we need to generate it in a standard way. One format that works across most CI Servers is JUnit XML.

### An Example

This functionality doesn't come built into RSpec, but it's simple enough to add through the use of a third-party library. There are plenty to choose from, but we'll go with ['rspec\_junit\_formatter'](https://github.com/sj26/rspec_junit_formatter).

After we install it we need to specify the formatter type and an output file for RSpec to consume. This is done through the use of two new arguments; `--format RspecJunitFormatter` and `--out results.xml`. But in order to use them we need to create a uniquely named XML output file for each test process. To accomplish that with `parallel_tests` we will need to take advantage of another feature in RSpec -- the `.rspec` file.

RSpec comes with the ability to place command arguments in a `.rspec` file within the root directory of our project, which will automatically be consumed at runtime. So we'll place our new arguments there, and use a kind of interpolation to inject an environment variable from `parallel_tests` which will be unique for each test process.

We're doing this instead of adding it to the `launch_in_parallel` method in our 'Rakefile' because of how parallel_tests handles execution. In order to get dynamically named XML output, we have to use the `.rspec` file. And we don't want our XML output to happen all the time. So let's make it conditional based on a new environment variable.

```
# filename: .rspec

<% if ENV['ci'] == 'on' %>
--format RspecJunitFormatter
--out tmp/result<%= ENV['TEST_ENV_NUMBER'] %>.xml
<% end %>
```

Now when we run our tests with the new environment variable set to 'on' (e.g., `ci='on' rake local:firefox`), each test process creates it's own result.xml by number (e.g., result.xml, result2.xml, etc.), and ends up in a `tmp` directory (`tmp` being short for temporary).

After a test run our directory structure should look like this:

```sh
.
|-- config
|   |-- cloud.rb
|   `-- local.rb
|-- .gitignore
|-- pages
|   |-- base_page.rb
|   |-- dynamic_loading.rb
|   `-- login.rb
|-- Rakefile
|-- .rspec
|-- spec
|   |-- dynamic_loading_spec.rb
|   |-- login_spec.rb
|   `-- spec_helper.rb
|-- tmp
|   |-- result2.xml
|   `-- result.xml
`-- vendor
    |-- chromedriver
    `-- the-internet
```

If we open one of the XML files from the `tmp` directory, it will contain a bunch of info from the test run. This is what our CI server will use to track test results (e.g., passes, failures, time to complete, etc.) and trend them over time.

Here's what one from a passing run looks like:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<testsuite tests="2" failures="0" errors="0" time="20.104438" timestamp="2014-02-01T19:22:05+00:00">
  <properties/>
  <testcase classname="spec.dynamic_loading_spec" name="Dynamic Loading Example 1: Hidden Element" time="10.926016"/>
  <testcase classname="spec.dynamic_loading_spec" name="Dynamic Loading Example 2: Rendered after the fact" time="9.177722"/>
</testsuite>
```

We don't want to commit the `tmp` directory or the XML files our repository, so let's make sure to add them to our ignore file.

```
# filename: .gitignore

*.log
tmp/
```

We're almost ready to wire things up to our CI Server, but before we do, we want to customize the failure messages in our tests.

## Part 3: Custom Failure Messaging

If we leave things as they are, we will receive messages that say 'expected true but got false' with no easy way to correlate the CI failure to the Sauce Labs job that it ran in -- which makes it hard to discern if the test failure was legitimate and painful to run down the real issue. Let's remedy that now.

### An Example

```ruby
# filename: spec/spec_helper.rb

require 'selenium-webdriver'
require_relative 'support/matchers'

RSpec.configure do |config|

  config.before(:each) do
    ...

    if ENV['host'] == 'saucelabs'
      $job_message = "Watch a video of the test at https://saucelabs.com/tests/#{@driver.session_id}"
    else
      $job_message = ""
    end
  end
  ...
```

First, we require a new file (which we need to create). It will contain our custom failure message (a.k.a. a custom matcher).

Next, in the `config.before(:each)` section of our `spec_helper`, just after the `@driver` has been instantiated, we add a conditional to set a global variable based on where the tests are being run. If Sauce Labs, then we create a message that includes the URL for the job being run. Otherwise, we set the message to be blank. Having the logic here means we only have to define it once, and can keep it out of our custom matchers.

In RSpec, we can create our own matchers (just like `be_true`). In them, we can specify custom failure messages. This is where we will take advantage of the `$job_message` variable.

```ruby
# filename: spec/support/matchers.rb

RSpec::Matchers.define :be_displayed do |expected|

  match do |actual|
    actual == true
  end

  failure_message_for_should do |actual|
    "Expected element to display, but it didn't. #{$job_message}"
  end

  failure_message_for_should_not do |actual|
    "Expected element to NOT display, but it did. #{$job_message}"
  end

end
```

We create a new `support` directory within our `spec` directory and add a `matchers.rb` file to hold our new `be_displayed` matcher. In our custom matcher we are doing 3 things:

1. Performing a comparison against the value being provided by our tests
2. Setting the custom failure message for the `.should` case
3. Setting the custom failure message for the `.should_not` case

In the custom failure messages, we are consuming the global variable that we created in our `spec_helper` file. For more info on custom matchers, check out [the write-up for them on the RSpec wiki](https://github.com/dchelimsky/rspec/wiki/Custom-Matchers).

Now we need to update our tests to use these matchers (replacing `be_true` with `be_displayed`).

```ruby
# filename: spec/dynamic_loading_spec.rb

...

  it 'Example 1: Hidden Element' do
    @dynamic_loading.example 1
    @dynamic_loading.start
    @dynamic_loading.finish_text?.should be_displayed
  end

  it 'Example 2: Rendered after the fact' do
    @dynamic_loading.example 2
    @dynamic_loading.start
    @dynamic_loading.finish_text?.should be_displayed
  end

end
```

```ruby
# filename: spec/login_spec.rb

...

  it 'succeeded' do
    @login.with('tomsmith', 'SuperSecretPassword!')
    @login.success_message?.should be_displayed
  end

  it 'failed' do
    @login.with('asdf', 'asdf')
    @login.failure_message?.should be_displayed
  end

  it 'forced failure' do
    @login.with('asdf', 'asdf')
    @login.failure_message?.should_not be_displayed
  end

end
```

Notice that the assertion helper methods used in our assertions have been shortened. This was done so that the tests read more intelligibly. We still want to keep the question mark though since it signifies that a boolean result is being returned.

Also, we added a new test in `login_spec.rb` titled `'forced failured'`. This helps us see what the failure output will look like.

All that's left to do now is update the page objects so our methods match our tests.

```ruby
# filename: pages/dynamic_loading.rb

...

  def finish_text?
    wait_for(10) { is_displayed? FINISH_TEXT }
  end

end
```

```ruby
# filename: pages/login.rb

  def success_message?
    is_displayed? SUCCESS_MESSAGE
  end

  def failure_message?
    is_displayed? FAILURE_MESSAGE
  end

end
```

Now when we run our test suite and a test fails, it will read with a helpful failure message, and a link to the Sauce Labs job.

> Expected element to NOT display, but it did. Watch a video of the test at https://saucelabs.com/tests/8b075a49cb20477f9aa820de4d196ac5

And if we need to add a different matcher, it's simple enough to do.

Now we're ready to wire things up to our CI server.

## Part 4: CI Server Job Configuration

The first thing we need to do (assuming you already have a CI Server setup) is create a job, name it appropriately (e.g., 'Shallow Tests IE8'), and configure it to meet the needs of you and your team.

If your team already uses CI and has a job that runs unit or integration tests, you can configure your job to key off of that one (e.g., if the unit tests pass, then run your shallow tests). This is ideal since their job is part of their workflow (e.g., it triggers every time code is pushed to the mainline), or there is some kind of time delayed polling (e.g., see if new code was pushed every 5 minutes). This means code is compiled and tests are run often.

Depending on the application you're testing, there may need to be an intermediate action that involves deploying code to a test environment which has test data. But by keying your job off of theirs, you are plugging your test suite directly into the development workflow. This is why running shallow tests to start is important -- we want only fast and critical tests to run to make sure they are providing valuable results.

If this is not how things are set up, then look to set up a job that polls the development codebase so that your tests can be triggered when there are code changes pushed by developers (e.g., with [post-receive hooks](http://git-scm.com/book/ch7-3.html#Server-Side-Hooks) or [on every pull request](http://brknthmb.com/post/75173073594/jenkins-using-the-github-pull-request-builder-plugin)).

Alternatively, you can set up scheduled runs for different times of the day or week. They're less ideal, and better suited for long running jobs during off-hours.

Let's step through an example showing you how to set up a job with [Jenkins](jenkins-ci.org).

## An Example

Jenkins can be set up on your own server or through a hosted offering like [CloudBees](http://www.cloudbees.com/jenkins). Once you have it set up with Ruby on the system (which is the default with CloudBees), you just need to install some plugins and set up a job.

In your job you can configure where to pull your tests from and when to run them.

Let's set up a job for our tests (pulling them in from a GitHub repository) and make them run after another job (e.g., a development build) completes successfully.

### Installing Plugins

1. From the Dashboard, click on `Manage Jenkins`
2. Then click on `Manage Plugins`
3. Once the Manage Plugins menu loads, click on the `Available` tab
4. In the top-right is a search field (labeled `filter`) -- input the name of what you want to install
5. In the list of plugins, find the one you want, and check the box to the left of its name
6. Click `Install without restart` at the bottom of the screen
7. On the next screen, check the box that says `Restart Jenkins when installation is complete and no jobs are running`

You should see a series of screens saying `Please wait while Jenkins is restarting` and `Please wait while Jenkins is getting ready to work`. When it's done, you'll be dropped back to the dashboard.

### Setting Up Your Test Job

#### Step 1: Create A Job

1. Click `New Job` from the top-left of the Dashboard
2. Give it a name (e.g., 'Shallow Tests IE8')
3. Select the `Build a free-style software project` option
4. Click `OK`

![jenkins new job](http://davehaeffner.com/selenium-guidebook/book-images/jenkins/1-job-setup/1-new-project.png)

This will take you to the configuration menu for the job you just created.

#### Step 2: Pull In Your Test Code

1. Scroll down to the `Source Code Management` section
2. Select the 'Git' option
3. Input the Repository URL (e.g., `git@github.com:tourdedave/selenium-guidebook-example.git`)
4. Specify the branch (e.g, `origin/master`)

![jenkins job config for git](http://davehaeffner.com/selenium-guidebook/book-images/jenkins/1-job-setup/2-git.png)

In order to use a private repository (like the example above) you need to set up deploy keys in GitHub. You can read more about that [here](https://help.github.com/articles/generating-ssh-keys).

#### Step 3: Set up Build Triggers

1. Scroll down to the `Build Triggers` section
2. Check `Build after other projects are built`
3. Input the name of the development project you want to key off of (e.g. 'the-internet develop')

![jenkins job config for build triggers](http://davehaeffner.com/selenium-guidebook/book-images/jenkins/1-job-setup/3-build-triggers.png)

#### Step 4: Build

1. Scroll down to the `Build` section
2. Click on `Add Build Step` and select `Execute Shell`
3. In the `Command` input box, add the following commands:

```sh
gem install bundler
bundle install
ci='on' rake cloud:ie['Windows XP','8']
```

![jenkins job config for running the build](http://davehaeffner.com/selenium-guidebook/book-images/jenkins/1-job-setup/5-execute-shell.png)

Since our tests have never run on this server, it's best to include the installation of the bundler gem (`gem install bundler`), running bundler to pull in our test suite's gems (`bundle install`), and then launching the test suite.

#### Step 5: Configure Test Reports

1. Scroll down to the `Post-build Actions` section
2. Click on `Add post-build action`
3. Select `Publish JUnit test result report`
4. In the `Test report XMLs` input box, type `tmp/result*.xml`
5. Click `Save`

![jenkins job config for consuming the xml test results](http://davehaeffner.com/selenium-guidebook/book-images/jenkins/1-job-setup/7-test-results-config.png)

After you click `Save` you will be taken to the job dashboard.

#### Step 6: Run Tests & View The Results

1. Click `Build Now` from the menu on the left
2. Once the build completes, click on `Latest Test Result` (or, click on the build from `Build History` and then click `Test Result`)
3. Click on the failed test listed under `All Failed Tests`

You should see an error message that states why the test failed along with a link to the job in Sauce Labs. Click the link if you want to watch a video of the failed test.

![jenkins job results](http://davehaeffner.com/selenium-guidebook/book-images/jenkins/2-job-report/3-test-result.png)

### Notifications

In order to make this job fully useful, we want to add notification to it. There are numerous ways to go about notifying team-members of failed builds (e.g., e-mail, chat, text, co-located visual cues, etc). Let's step through setting up a simple chat notification with HipChat.

#### Step 1: System Setup

1. Install the HipChat Jenkins Plugin
2. Click on `Manage Jenkins` from the main dashboard
3. Click on `Configure System`
4. Scroll down to `Global HipChat Notifier Settings`
5. Input your HipChat API Token into the `API Token` input field (which you can get from your Admin API Tokens page)
6. Input the chat room you would like to send the notifications to in the `Room` input field (this can be the name of the room (case-sensitive) or the ID of the room)
7. Input the URL of your Jenkins server into the `Jenkins URL` input field (this will be used in the notifications)
8. Click `Save`

This will take you back to the main dashboard page.

#### Step 2: Configure Job Notifications

1. Click on the job you want to add chat notifications to (e.g., `Shallow Tests IE8`)
2. Click `Configure` (along the left side menu)
3. Scroll down to `Post-build Actions`
4. Click `Add post-build action`
5. Select `HipChat Notifications`
6. Click `Save`

Now when your tests run (either by manually triggering them, or from being triggered by another job) you will receive the end-result in the chat room you specified.

![hipchat notification received](http://davehaeffner.com/selenium-guidebook/book-images/jenkins/3-notifications/hipchat-notification-received.png)

The only unfortunate thing about this configuration is that it will send notifications for both test passes and test failures regardless of the job's stability. So it may be a bit too noisy for your colleagues. If that's the case, then consider looking into e-mail notifications (which are also available in the `Post-build Actions` section).

## Closing Thoughts

By using a CI Server you're able to put your tests to work by using computers for what they're good at -- freeing you up to focus on more important things. Keep in mind that there are numerous ways to configure your CI server. Be sure to find what works best for you and your team by testing and learning from a couple of different configurations. It's well worth the effort.
